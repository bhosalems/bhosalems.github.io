<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://bhosalems.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bhosalems.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-02T06:05:44+00:00</updated><id>https://bhosalems.github.io/feed.xml</id><title type="html">Mahesh’s webpage</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Distance metric for fairness in vision language models</title><link href="https://bhosalems.github.io/blog/2024/kl_distance/" rel="alternate" type="text/html" title="Distance metric for fairness in vision language models"/><published>2024-11-28T14:15:00+00:00</published><updated>2024-11-28T14:15:00+00:00</updated><id>https://bhosalems.github.io/blog/2024/kl_distance</id><content type="html" xml:base="https://bhosalems.github.io/blog/2024/kl_distance/"><![CDATA[<h3 id="section-1-distance-metrics">Section 1: Distance metrics</h3> <p>In contrastive learning distance metric is used to bring the postive pairs closer and push negative pairs farther apart. Distance metric therefore should be chosen carefully. In case of probability distributions we will investigate whether KL divergence is good metric to use in the loss function. Distance metrics <span style="font-weight: bold;">MUST</span> satify below properties,</p> <ol> <li><span style="font-weight: bold;">Non-Negativity</span>: \(D(P, Q) &gt;= 0\).</li> <li><span style="font-weight: bold;">Identity of Indiscrenibles</span>: \(D(P, Q) = 0\) iff \(P==Q\).</li> <li><span style="font-weight: bold;">Symmetry</span>: \(D(P, Q) = D(Q, P)\). Distance should not depend on the direction.</li> <li><span style="font-weight: bold;">Triangle Inequaltiy</span>: \(D(P, Q) &lt;= D(P, R) + D(R, Q)\). Direct distance is the shortest distance.</li> </ol> <hr/> <h3 id="section-2-kl-diveregence">Section 2: KL Diveregence</h3> <p>The Kullback–Leibler (KL) divergence between two probability distributions is given by,</p> \[D_{KL}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}\] <h5 id="21-tying-with-information">2.1. Tying with Information</h5> <p>In information theory, <span style="font-weight: bold;">information</span> quantifies uncertainty reduction. For an event \(x\) with probability \(P(x)\), the information content is:</p> \[I(x) = -\log P(x)\] <p>Rare events contain more information than frequent events. The <span style="font-weight: bold;">entropy</span> of a distribution \(P\) is the expected information content:</p> \[H(P) = - \sum_x P(x) \log P(x)\] <p>KL Divergence represents the additional information required to describe samples from \(P\) using \(Q\):</p> \[D_{KL}(P \| Q) = \sum_x (P(x) \log P(x) - P(x) \log Q(x))\] <ul> <li>\(D_{KL}(P \| Q)\) measures how poorly \(Q\) approximates \(P\).</li> <li>High divergence indicates significant <span style="font-weight: bold;">information mismatch</span>.</li> </ul> <p>Therefore KL diveregence lacks the <span style="font-weight: bold;">geometric</span> intepretation of the distance and violates symmetric and triangle-inequlaity requirement i.e</p> \[D_{KL}(P \| Q) \neq D_{KL}(Q \| P)\] <p>and for example: If \(D_{KL}(P \| Q)\) is large but \(D_{KL}(Q \| R)\) is small, the sum \(D_{KL}(P \| Q) + D_{KL}(Q \| R)\) might not meaningfully bound \(D_{KL}(P \| R)\).</p> <hr/> <h3 id="section-3-sinkhorn-distance">Section 3: Sinkhorn Distance</h3> <p>The Sinkhorn Distance is a regularized variant of the Wasserstein Distance, designed for computational efficiency and smoothness. It is defined as:</p> \[W_{\epsilon(D_B, D_{B_a})} = \inf_{\gamma \in \Gamma(D_B, D_{B_a})} \mathbb{E}_{(p, q) \sim \gamma}[c(p, q)] + \epsilon H(\gamma \| \mu \otimes \nu)\] <p>Where:</p> <ul> <li> <p>\(\Gamma(D_B, D_{B_a})\): The set of all joint distributions \(\gamma(p, q)\) such that:</p> \[\int \gamma(p, q) dq = D_B(p) \quad \text{first marginal matches } D_B,\] \[\int \gamma(p, q) dp = D_{B_a}(q) \quad \text{second marginal matches } D_{B_a}.\] </li> <li>\(c(p, q)\): The transport cost (e.g., Euclidean distance) between points \(p\) and \(q\).</li> <li>\(\epsilon\): A regularization parameter controlling the weight of the entropy term.</li> <li>\(H(\gamma \| \mu \otimes \nu)\): The relative entropy between \(\gamma\) and the product measure \(\mu \otimes \nu\).</li> </ul> <h5 id="31-key-components">3.1. Key Components</h5> <h6 id="joint-distribution-gamma">Joint Distribution \(\gamma\)</h6> <ul> <li>\(\gamma(p, q)\) represents a joint probability distribution over \(p\) and \(q\), where: <ul> <li>\(p\) is sampled from \(D_B\) (general batch distribution).</li> <li>\(q\) is sampled from \(D_{B_a}\) (subgroup-specific distribution).</li> </ul> </li> <li><strong>Marginal Constraints</strong>: <ul> <li>The first marginal of \(\gamma\) is \(D_B\).</li> <li>The second marginal of \(\gamma\) is \(D_{B_a}\).</li> </ul> </li> </ul> <h6 id="transport-cost-mathbbe_p-q-sim-gammacp-q"><strong>Transport Cost \(\mathbb{E}_{(p, q) \sim \gamma}[c(p, q)]\)</strong></h6> <ul> <li>This term minimizes the cost of moving mass between \(D_B\) and \(D_{B_a}\).</li> <li>Alignment is achieved by finding an optimal \(\gamma\) that connects \(D_B\) and \(D_{B_a}\) while minimizing this cost.</li> </ul> <h6 id="regularization-term-hgamma--mu-otimes-nu"><strong>Regularization Term \(H(\gamma \| \mu \otimes \nu)\)</strong></h6> <ul> <li>\(\mu \otimes \nu\) is a general measure, not necessarily a probability measure.</li> <li>Regularization encourages \(\gamma\) to have a smoother distribution, influenced by \(\mu \otimes \nu\).</li> </ul> <hr/> <h5 id="32-role-of-gamma-in-fairness-learning">3.2. Role of \(\gamma\) in Fairness Learning</h5> <p>In fairness learning, the Sinkhorn Distance aligns the distributions \(D_B\) (general) and \(D_{B_a}\) (specific to a subgroup). The joint distribution \(\gamma\):</p> <ol> <li> <p><strong>Ensures Marginal Consistency</strong>:</p> <ul> <li>\(\int \gamma(p, q) dq = D_B(p)\).</li> <li>\(\int \gamma(p, q) dp = D\_{B_a}(q)\).</li> </ul> </li> <li> <p><strong>Balances Objectives</strong>:</p> <ul> <li>Align \(D_B\) and \(D_{B_a}\) via transport cost minimization.</li> <li>Smooth the distribution \(\gamma\) via entropy regularization.</li> </ul> </li> <li> <p><strong>Optimal Transport Plan</strong>:</p> <ul> <li>\(\gamma(p, q)\) determines how much “mass” is moved from \(p\) (in \(D_B\)) to \(q\) (in \(D_{B_a}\)).</li> </ul> </li> </ol> <hr/> <h5 id="33-why-sinkhorn-distance">3.3. Why Sinkhorn Distance?</h5> <p>Compared to KL Divergence:</p> <ol> <li><strong>Handles Marginal Constraints</strong>: <ul> <li>Ensures the distributions \(D_B\) and \(D_{B_a}\) are matched via the transport plan \(\gamma\).</li> </ul> </li> <li><strong>Incorporates Regularization</strong>: <ul> <li>The term \(H(\gamma \| \mu \otimes \nu)\) smooths \(\gamma\), making computation efficient and avoiding degenerate solutions.</li> </ul> </li> <li><strong>True Metric</strong>: <ul> <li>Unlike KL Divergence, Sinkhorn Distance satisfies all the properties of a true metric (e.g., triangle inequality).</li> </ul> </li> </ol> <hr/> <div id="disqus_thread"></div> <script type="text/javascript">var disqus_shortname="bhosalems-github-io";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>]]></content><author><name></name></author><category term="technical"/><category term="kl-diveregence"/><category term="self-learning"/><summary type="html"><![CDATA[Distance metric for fairness]]></summary></entry><entry><title type="html">Diffusion Models Background and Code</title><link href="https://bhosalems.github.io/blog/2024/diff/" rel="alternate" type="text/html" title="Diffusion Models Background and Code"/><published>2024-11-28T09:15:00+00:00</published><updated>2024-11-28T09:15:00+00:00</updated><id>https://bhosalems.github.io/blog/2024/diff</id><content type="html" xml:base="https://bhosalems.github.io/blog/2024/diff/"><![CDATA[<p>We start by summarizing the diffusion models and guidance to control the generation process.</p> <h2 id="section-1-background">Section 1: Background</h2> <hr/> <h5 id="11-diffusion-models">1.1. Diffusion Models</h5> <p>Diffusion Models (DMs) <a class="citation" href="#Ho2020Diffusion">(Ho et al., 2020)</a> generate data samples by gradually adding noise to data through a forward diffusion process, followed by a reverse denoising process that reconstructs the original sample.</p> <p>The forward process corrupts a data sample \(x_0\) through iterative noise addition controlled by a schedule \(\alpha = \{\alpha_t\}*{t=1}^T\):</p> \[q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \alpha_t} \, x_{t-1}, \alpha_t I)\] <p>With \(\bar{\alpha}_t = \prod_{i=1}^{t} (1 - \alpha_i)\), \(x_t\) can be computed from \(x_0\) with marginal distribution:</p> \[q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} \, x_0, (1 - \bar{\alpha}_t) I)\] <p>The reverse process, parameterized by a neural network \(p_\theta\), learns to reconstruct the data sample by predicting the denoised mean and variance at each step:</p> \[p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\] <p>Latent Diffusion Models (LDMs) <a class="citation" href="#stablediffusion">(Rombach et al., 2021)</a> work in a compressed latent space \(z_t\), rather than the high-dimensional data space, improving efficiency. The data \(x_0\) is encoded as \(z_0\) through an autoencoder, and the diffusion process is applied in this latent representation.</p> <p>LDM models learn to minimize the objective:</p> \[L = \mathbb{E}_{z_0, t, \epsilon \sim \mathcal{N}(0, 1)} \left[ \| \epsilon - \epsilon_\theta(z_t, t) \|_2^2 \right]\] <p>where \(\epsilon\) is random Gaussian noise, and \(\epsilon_\theta\) is the model’s predicted noise at time \(t\).</p> <h5 id="12-guidance">1.2. Guidance</h5> <p>In the denoising process, different conditional inputs \(c\) (text, image, depth, mask, etc.) can be added to control the generation; so the denoising model predicts \(\epsilon_\theta(z_t, t, c)\).</p> <p>At the time of sampling, the diffusion score \(\epsilon_\theta(z_t, t, c)\) is modified to include the adversarial gradient of the classifier <a class="citation" href="#NEURIPS2021_49ad23d1">(Dhariwal &amp; Nichol, 2021)</a>:</p> \[\tilde{\epsilon}_\theta(z_t, t, c) = \epsilon_\theta(z_t, t) - w \nabla_{z_t} \log p_\phi(c | z_t)\] <p>where \(w\) is a guidance strength parameter that controls the influence of the classifier.</p> <p>In classifier-free guidance <a class="citation" href="#classifier_free">(Ho &amp; Salimans, 2022)</a>, a single neural network is used to parameterize both the unconditional denoising diffusion model \(p\theta(z)\) and the conditional denoising diffusion model \(p\theta(z | c)\). While training, the unconditional model receives a null token, \(\Phi\), as \(c\) randomly with some probability \(p\_\text{uncond}\), set as a hyperparameter.</p> <p>During sampling, a linear combination of conditional and unconditional score estimates is used:</p> \[\tilde{\epsilon}_\theta(z_t, t, c) = (1 + w) \epsilon_\theta(z_t, t, c) - w (\epsilon_\theta(z_t, t))\] <h2 id="section-2-code">Section 2: Code</h2> <hr/> <p>We will use the codebase of DDPM-IP <a class="citation" href="#ning2023input">(Ning et al., 2023)</a> to explore this section.</p> <h5 id="21-forward-diffusion-noising-process">2.1. Forward Diffusion (Noising Process)</h5> <h6 id="step-1-noise-addition"><span style="font-weight: bold;">Step 1: Noise Addition</span></h6> <p>The forward diffusion process incrementally adds Gaussian noise to the data:</p> \[q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \alpha_t} \cdot x_{t-1}, \alpha_t \cdot I)\] <h6 id="direct-sampling-of-x_t">Direct Sampling of \(x_t\)</h6> <p>The noise is accumulated across timesteps, allowing \(x_t\) to be sampled directly from \(x_0\):</p> \[q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} \cdot x_0, (1 - \bar{\alpha}_t) \cdot I)\] <h6 id="code-implementation">Code Implementation:</h6> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alphas</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">betas</span>
<span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">sqrt_alphas_cumprod</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">sqrt_one_minus_alphas_cumprod</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">alphas_cumprod</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">q_sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">th</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span>
        <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_start</span>
        <span class="o">+</span> <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_one_minus_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
    <span class="p">)</span>
</code></pre></div></div> <h5 id="22-reverse-diffusion-denoising-process">2.2 Reverse Diffusion (Denoising Process)</h5> <h6 id="step-2-predicted-noise-epsilon_thetax_t-t"><span style="font-weight: bold;">Step 2: Predicted Noise \(\epsilon_{\theta}(x_t, t)\)</span></h6> <p>The model directly predicts the noise added to \(x_t\). This prediction is denoted by \(\epsilon_\theta(x_t, t)\) and represents the learned noise.</p> \[\epsilon_\theta(x_t, t) = f_\theta(x_t, t)\] <h6 id="code-implementation-1">Code Implementation:</h6> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">_scale_timesteps</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="o">**</span><span class="n">model_kwargs</span><span class="p">)</span>
</code></pre></div></div> <h6 id="step-3-predicted-x_0"><span style="font-weight: bold;">Step 3: Predicted \(x_0\)</span></h6> <p>Using the predicted noise \(\epsilon_\theta(x_t, t)\), the noiseless image \(x_{\text{pred}_0}\) (predicted \(x_0\)) is reconstructed. This step removes the noise from \(x_t\) using the diffusion schedule.</p> \[x_{\text{pred}_0} = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\] <h6 id="code-implementation-2">Code Implementation:</h6> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_predict_xstart_from_eps</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span>
        <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_recip_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_t</span>
        <span class="o">-</span> <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sqrt_recipm1_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">eps</span>
    <span class="p">)</span>
</code></pre></div></div> <h6 id="step-4-predicted-mean-mu_thetax_t-t-"><span style="font-weight: bold;">Step 4: Predicted Mean \(\mu_\theta(x_t, t)\) </span></h6> <p>The mean of the reverse process \(\mu_\theta(x_t, t)\) is calculated based on the predicted \(x_0\). This is a crucial step in approximating the denoising process.</p> \[\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \cdot \left(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \cdot x_{\text{pred}_0}\right)\] <h6 id="code-implementation-3">Code Implementation:</h6> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">q_posterior_mean_variance</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">posterior_mean</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">posterior_mean_coef1</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_start</span>
        <span class="o">+</span> <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">posterior_mean_coef2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_t</span>
    <span class="p">)</span>
    <span class="n">posterior_variance</span> <span class="o">=</span> <span class="nf">_extract_into_tensor</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">posterior_variance</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">posterior_mean</span><span class="p">,</span> <span class="n">posterior_variance</span>
</code></pre></div></div> <h6 id="step-5-adding-stochastic-noise-"><span style="font-weight: bold;">Step 5: Adding Stochastic Noise </span></h6> <p>Finally, stochastic noise is added to the predicted mean \(\mu_\theta(x_t, t)\) to sample \(x_{t-1}\). This accounts for the uncertainty in the reverse process, ensuring the model generates diverse outputs.</p> \[x_{t-1} = \mu_\theta(x_t, t) + \sqrt{\Sigma_\theta(x_t, t)} \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\] <h6 id="code-implementation-4">Code Implementation:</h6> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="n">nonzero_mask</span> <span class="o">*</span> <span class="n">th</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">out</span><span class="p">[</span><span class="sh">"</span><span class="s">log_variance</span><span class="sh">"</span><span class="p">])</span> <span class="o">*</span> <span class="n">noise</span>
</code></pre></div></div> <p>That briefly settles diffusion models, for more details please read <a class="citation" href="#chan2024tutorial">(Chan, 2024)</a>.</p> <h2 id="references">References:<br/></h2> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="chan2024tutorial" class="col-sm-8"> <div class="title">Tutorial on Diffusion Models for Imaging and Vision</div> <div class="author"> Stanley H Chan </div> <div class="periodical"> <em>arXiv preprint arXiv:2403.18103</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="ning2023input" class="col-sm-8"> <div class="title">Input Perturbation Reduces Exposure Bias in Diffusion Models</div> <div class="author"> Mang Ning,&nbsp;Enver Sangineto,&nbsp;Angelo Porrello, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Simone Calderara, Rita Cucchiara' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In International Conference on Machine Learning</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="classifier_free" class="col-sm-8"> <div class="title">Classifier-Free Diffusion Guidance</div> <div class="author"> Jonathan Ho,&nbsp;and&nbsp;Tim Salimans </div> <div class="periodical"> <em></em> 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="NEURIPS2021_49ad23d1" class="col-sm-8"> <div class="title">Diffusion Models Beat GANs on Image Synthesis</div> <div class="author"> Prafulla Dhariwal,&nbsp;and&nbsp;Alexander Nichol </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="stablediffusion" class="col-sm-8"> <div class="title">High-Resolution Image Synthesis with Latent Diffusion Models</div> <div class="author"> Robin Rombach,&nbsp;Andreas Blattmann,&nbsp;Dominik Lorenz, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Patrick Esser, Björn Ommer' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em></em> 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Ho2020Diffusion" class="col-sm-8"> <div class="title">Denoising Diffusion Probabilistic Models</div> <div class="author"> Jonathan Ho,&nbsp;Ajay Jain,&nbsp;and&nbsp;Pieter Abbeel </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <p><br/></p> <hr/> <div id="disqus_thread"></div> <script type="text/javascript">var disqus_shortname="bhosalems-github-io";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>]]></content><author><name></name></author><category term="technical"/><category term="diffusion"/><category term="self-learning"/><summary type="html"><![CDATA[Diffusion models and its code in Gaussian diffusion codebase]]></summary></entry><entry><title type="html">SoccerNet Player Re-identification</title><link href="https://bhosalems.github.io/blog/2022/sn-reid/" rel="alternate" type="text/html" title="SoccerNet Player Re-identification"/><published>2022-08-20T09:15:00+00:00</published><updated>2022-08-20T09:15:00+00:00</updated><id>https://bhosalems.github.io/blog/2022/sn-reid</id><content type="html" xml:base="https://bhosalems.github.io/blog/2022/sn-reid/"><![CDATA[<p>Last semester as a part of CSE 610 Sports Video Analytics class, we worked on the <a href="https://www.soccer-net.org/tasks/re-identification">Soccernet-Player Re-identification</a> challenge. Below are the notes from the work done in this project.</p> <h4 id="whats-the-task-of-person-re-identification"><strong>What’s the task of Person Re-identification?</strong></h4> <p>The task of Person Re-identification can be formulated differently leading to multiple definitions. I will start with one which is quite straightforward, and introduce others later. As name suggests it is all about “Re” identifying the person. More precisely, person Re-identification is a task of identifying the same person in two time and/or view disjoint frames taken from multiple cameras. <br/></p> <p>Below is an image from person re-identification dataset called Market-1501<a class="citation" href="#zheng2015scalable">(Zheng et al., 2015)</a>. It contains in total 8 sequence of images(3 sequences in first two rows and two sequences in the last row. Note, here sequence doesn’t necessarily mean any order between images, it just is a collection), each image in a sequence is of the same person/identity taken from multiple views captured by different cameras in a market. The task of person re-identification is to build the correspondence between images in the same sequence. Also, as can be seen in the laarketst row, there can also be negative examples i.e. there is either no sufficient information to identify the person or there’s no more images to retrieve (only single reference image). Based on requirements of a task at hand, output in such cases could be different e.g. “none” if similarity score/ other metric is below some reasonable threshold value.<br/></p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/Market-1501.png" width="900"/> </p> </html> <p><em><center>Market 1501 Dataset <a class="citation" href="#zheng2015scalable">(Zheng et al., 2015)</a>.</center></em> <br/> Having gone over what person re-identification is, player re-identification is self-explanatory : person re-identification done for players in particular sports.</p> <p>Mostly all publicly available person/player re-identification datasets have cropped images of players from image frames in the video. These videos can be from different geographic locations and timestamps, captured from multiple cameras with dissimilar views/orientations. Although timestamps are different, difference is smaller (in minutes or less). Multiple camera views pose a challenge because the views are disjoint, temporal distance between images is not constant, lighting conditions and backgrounds are different.</p> <p>Given such dataset, you will find some standard definitions in literature which I will introduce here. <br/></p> <ul> <li>Anchor/Query Image : An image of the target player to be re-identified.<br/></li> <li>Action frame : A frame from which an anchor image is captured. In case of SoccerNet dataset, action signifies some interesting event in the soccer e.g. a goal. All the frames in the video for that action are grouped together and used for evaluation as these frames are temporally closer.<br/></li> <li>Reference frame: All other frames (can be from same or different action) in the dataset except the action frame.<br/></li> <li>Gallery set: Nothing fancy, it simply is a set of all reference frames / bounding boxes(likely have different views/timestamps).<br/></li> <li>Positive Image: An image of the same identity as that in the anchor image.<br/></li> <li>Negative Image: An image which doesn’t have the same identity as that in the anchor image.<br/></li> </ul> <p>Below is the illustration of Soccernet-v3 dataset -<br/></p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/soccernet-v3-reid-illustration.png" width="950" height="600"/> </p> </html> <p><em><center>Soccernet Re-identification Dataset Illustration</center></em> <br/> Let’s say above image is divided into four parts by two dotted lines. Top left corner is an action frame, most likely a goal. After that, in top right you have reference frames named as replay frames; notice the small temporal distance between action and replay frames. In the lower-left part of the image, 18 bounding boxes are captured, each will be used as (possible) query image, and all the 37 bounding boxes captured from the reference images will form a gallery, shown in the lower right part. In some cases, such as the bottom-most image of a referee in the queries doesn’t have any matching image in the gallery set, then it will be moved to gallery set to create a distraction. Having taken a look at some images in the dataset, take a minute to think about the challenges I mentioned in the dataset such as difference in background, resolution and size.</p> <p>So in summary, an anchor image is taken from action frames; positive images and negative images are from gallery set. You are given an anchor image, your model needs to find the positive image from the gallery set. With above description you could also formulate the problem of re-identification in terms of image retrieval based on metric learning. I will talk more about metric learning in later section.</p> <h4 id="state-of-the-art-methods-of-re-identification"><strong>State-of-the-Art methods of Re-identification</strong></h4> <p>Current methods of player re-identification mainly focus on two ways - one is to get high quality discriminative features and other is to define the distance metric which can be used as a loss for learning task effectively. I will talk briefly about the prior as it aligned with the requirements of the class. While learning the features from the images, it is important to work on relatively similar scale of the images. In more open-world settings, distance of objects from the camera is different, and so ae there sizes in the image; although to some extent it is taken care by cropping the image and resizing the images to same size, it is important to construct features from different scales in an image. One of such methods exploring the ideas of multiple scales is from the authors of OSNET<a class="citation" href="#zhou2019osnet">(Zhou et al., 2019)</a>. OsNET was also SOTA method in <a href="https://github.com/SoccerNet/sn-reid/tree/main/torchreid/models">SoccerNet baselines</a> which gave the best results. One of the other methods that we reviewed was recent addition to SOTAs, a transformer based re-identification model - TransReid. Let me describe both OSNET and TransReid in following sections.</p> <p><strong>1. Omni-Scale Feature Learning for Person Re-Identification (OSNET)</strong> <a class="citation" href="#zhou2019osnet">(Zhou et al., 2019)</a><br/> Authors of this paper argue that to match the people, small local features (e.g. shoes, purse etc.) and relatively larger global features (e.g. whole body appearance) are equally important. Therefore, such discriminative features should be <em>omniscale</em>, defined as the combination of variable homogeneous scales and heterogeneous scales, each of which is composed of a mixture of multiple scales.</p> <p>Authors propose a novel CNN architecture OSNET. The main idea is to have multiple CNN streams with different receptive fields so that the multiscale features can be learnt. At last, resulting multiscale feature maps from each stream are fused by weighted aggregation gate (AG). The AG is a mini-network sharing parameters across all the CNN streams. With the trainable AG, the generated channel-wise weights become input-dependent, hence the dynamic scale fusion. There are some more ideas adapted in this paper such as <a href="http://stanford.edu/class/ee367/Winter2019/bergman_report.pdf">depth-wise convolutions</a> to make the module light-weight. For more detailed understanding reader is advised to review OSNET paper <a class="citation" href="#zhou2019osnet">(Zhou et al., 2019)</a>.</p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/osnet-arch.png"/> </p> </html> <p><em><center>OSNET bottleneck block</center></em> <br/></p> <p><strong>2. TransReid: Transformer-based Object Re-Identification</strong> <a class="citation" href="#https://doi.org/10.48550/arxiv.2102.04378">(He et al., 2021)</a><br/> One of the other methods which although, doesn’t directly encode multiscale features it does address main problems with CNN based Re-ID methods. I have seen TransReid perform quite well in the task of player re-identification, which should come with no surprise as Transformer based models are performing better and better stacking up hundreds of submissions in the top conferences these days.</p> <p>There are two main problems with the traditional CNN based approaches of re-identification - 1. CNN based methods focus on small discriminative features due to a Gaussian distribution of effective receptive fields. 2. Down-sampling operators of CNN reduce the spatial dimension of the feature-map (as you would also see that this was one of the motivations for us to use Layer-wise similarity discussed in the later section).</p> <p>Authors of TransReid propose to address these issues -<br/> Use of attention captures long range dependencies as complete global information is available at each layer despite its depth. Without down-sampling operators, transformers can keep more detailed information. To further add robust features authors introduce two modules -</p> <ol> <li>Jigsaw patches module : As with vision transformer, <a class="citation" href="#https://doi.org/10.48550/arxiv.2010.11929">(Dosovitskiy et al., 2020)</a> the image is split into fixed sized patches and attention based mechanism is used to learn the features. This module attempts to rearrange the patch embeddings via shift and shuffle operations and regroup them for further feature learning. This enables robustness in the learned features and also expands on long-range dependencies.</li> <li>Side information embedding : In many of the re-identification datasets we have non-visual information which can not be processed by purely CNN based model. Therefore, there is no way of addressing data bias brought by cameras or viewpoints. This module, similar to position encoding in vision transformer, uses learnable 1D embeddings to encode side information suh as camera and view metadata.</li> </ol> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/trans-reid-arch.png" width="900"/> </p> </html> <p><em><center>TransReid architecture</center></em> <br/></p> <p>For more detailed treatment of TransReid reader is advised to review TransReid paper <a class="citation" href="#https://doi.org/10.48550/arxiv.2102.04378">(He et al., 2021)</a></p> <h4 id="appearance-and-pose-as-discriminative-features"><strong>Appearance and Pose as discriminative features</strong></h4> <p><strong>Motivation:</strong><br/> As compared to the task of person re-identification, the task of player re-identification is significantly challenging. Many methods base their model on appearance as discriminative feature to learn the metrics, but in case of players, appearance of almost all is similar - for example in a game of football the general physique of all the players would be on average similar. Almost all players from same team will wear similar jersey, exception being goal-keepers, but there is only one goalkeeper in a team on the field. Now, you might be able to identify player based on their jersey numbers but remember we have to re-identify players from different camera views, and it is more likely than not that the jersey numbers are either not visible in the given view or too obscure to even be detected let alone be identified as can be seen from below pictures.</p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/obscure_imgs.png" width="100" height="200"/> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/obscure_imgs1.png" width="100" height="200"/> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/obscure_imgs2.png" width="100" height="200"/> </p> </html> <p><em><center>Soccernet-v3 images with obscure Jersey numbers </center></em> <br/> Therefore, appearance features alone are not sufficient. It is also evident from the difference in the performance of the SOTA methods such as OSNET on person re-identification vs player re-identification.<br/><br/></p> <div style="display: flex; justify-content: center;"> <table style="border-collapse: collapse; width: 50%; text-align: center;"> <thead> <tr> <th style="border: 1px solid #ddd; padding: 8px;">Dataset</th> <th style="border: 1px solid #ddd; padding: 8px;">mAP (%)</th> <th style="border: 1px solid #ddd; padding: 8px;">Rank-1 (%)</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Person re-identification (Market1501)</td> <td style="border: 1px solid #ddd; padding: 8px;">81</td> <td style="border: 1px solid #ddd; padding: 8px;">93.6</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Player re-identification (SoccerNet-v3)</td> <td style="border: 1px solid #ddd; padding: 8px;">61.6</td> <td style="border: 1px solid #ddd; padding: 8px;">51.2</td> </tr> </tbody> </table> </div> <p><em><center>Performance of SOTA OSNET on person vs player re-identification (mean average precision and Rank-1 accuracy)<a class="citation" href="#zhou2019osnet">(Zhou et al., 2019)</a>. </center></em></p> <p>One of the main difference we identified in traditional person re-identification datasets and SoccerNet dataset is that temporal distance between anchor image and reference images in case of SoccerNet dataset is much smaller than that of person re-identification datasets. Which means a player in anchor image and the same player in (positive) reference images is likely to have similar body posture. Also, in almost every team-sport, based on the role of a player in overall game, there are distinct moves that they do at a given time. Posture of players therefore, could be used as additional discriminative feature to guide the task of metric learning. This was one of the main ideas that we implemented in the project.</p> <p><strong>Methodology:</strong><br/> We need to extract both posture features and appearance features from the input image; We use two-stream model where one stream is called as appearance extractor which works on extracting the appearance features from the images, and second stream called as part/pose extractor works on extracting the pose related features from the images. We use RESNET-50 as appearance extractor and sub-model of openpose as pose-extractor. At the end we need to combine both the appearance and pose features to calculate the final loss. We use (compact) bi-linear pooling to pool the features from both the streams. Our choice of pose extractor and pooling has been adapted from Part-aligned bilinear pooling for re-identification paper<a class="citation" href="#Suh_2018_ECCV">(Suh et al., 2018)</a>.</p> <p>Below image shows the two-stream extractor -<br/></p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/model_arch.png" width="900"/> </p> </html> <p><em><center>Two stream (appearance and pose) extractor model architecture. </center></em></p> <p>Note that we use two losses, first is Triplet loss as a similarity loss which will be explained in detail in a later section. Another is an identity loss, this is nothing but a traditional cross-entropy loss used in classification tasks. It is formally given as,</p> \[L = \frac{1}{m} \sum_{i=1}^m y_i \dot{} \log{\hat {y}_i}\] <p><strong>OpenPose:</strong><br/> Let me briefly describe main concepts in OpenPose and the sub-model that we use in our work. OpenPose<a class="citation" href="#DBLP:journals/corr/abs-1812-08008">(Cao et al., 2018)</a> is the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints (total 135 keypoints).</p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/pose_face_hands.gif"/> </p> <p style="text-align:center;"><em>Authors of OpenPose: Ginés Hidalgo (left) and Hanbyul Joo (right) in front of the CMU Panoptic Studio.<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose"> image source</a></em></p> </html> <p>There are mainly two approaches in multi-person 2D human pose detection - <br/></p> <ol> <li>Top-down approach - In top-down approach, a single person is detected first, and then the pose is estimated for every such detection.</li> <li>Bottom-up approach - On the contrary, in bottom-up approach, local features (such as body parts) are detected and associated with each other to get the global context/information about complete pose.</li> </ol> <p>OpenPose is based off Bottom-up approach. Figure below visualizes the complete pipeline of the OpenPose.</p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/Openpose_pipeline.png" width="900"/> </p> </html> <p><em><center>OpenPose pipeline <a class="citation" href="#DBLP:journals/corr/abs-1812-08008">(Cao et al., 2018)</a></center></em></p> <p>OpenPose takes in 2D color image of size $H \times W$ as input (Fig. a) and produces anatomical key points on each person in the image as output (Fig. e). First, feed-forward network predicts set of confidence maps $S$ of body parts (Fig. b) and set of 2D vector fields $L$ called as part affinity fields (PAF), which encode degree of association between body parts (Fig. c). The set $S=(S_1, S_2, …, S_J)$ has $J$ confidence maps, one per part, where $S_j \in \mathbb{R}^{w \times h}$, $j \in {1 . . . J}$. The set $L=(L_1,L_2, …,L_C )$ has $C$ vector fields one per limb (including face, although technically it’s not a limb) where $L_c \in \mathbb{R}^{w \times h \times 2}$, $c \in {1, ..C}$. Once all the PAFs and confidence maps are identified, bipartite matching does the association and the result is 2D key-points for all people in the image (Fig. d). Note that each image location in L encodes a 2D vector as shown in the below figure.</p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/PAFs.png"/> </p> </html> <p><em><center> (left) Part Affinity Fields (PAFs) corresponding to the limb connecting right elbow and wrist. The color encodes orientation. (right) A 2D vector in each pixel of every PAF encodes the position and orientation of the limbs. <a class="citation" href="#DBLP:journals/corr/abs-1812-08008">(Cao et al., 2018)</a></center></em></p> <p>As stated earlier, we only need a sub-model of the OpenPose, particularly, the part until it calculates the final part confidence features which we use for bi-linear pooling with appearance features. Multi-stage architecture of OpenPose is given in the below figure, where first stages predict PAFs and later stages predict the part confidence maps.</p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/OpenPose_arch.png"/> </p> </html> <p><em><center> Multistage OpenPose Architecture</center></em></p> <p>First, image is fed into pretrained VGG-19, which gives the feature maps $F$ that is input to the first stage and outputs the first PAF. In each subsequent stage, the predicted PAF from the previous stage and the original image feature map $F$ are concatenated and used to produce the refined predictions. Formally first PAF $L^1$ is calculated as,</p> \[L^1 = \phi^1(F)\] <p>Subsequent PAFs are calculated as,</p> \[L^t = \phi^t(F, L^{t-1}), \forall 2\le t \le T_p\] <p>where, $\phi^t$ refers to CNNs inference at stage $t$. After total of $T_p$ PAF stages, last PAF is given as input to next stage for estimating part confidence map. First stage only takes $L^{T_p}$ and $F$ as inputs, i.e.</p> \[S^{T_p} = \rho^t(F, L^{T_p}), \forall t=T_p\] <p>whereas subsequent stages take $L^{T_p}$, $F$ and $S^{T-1}$ as inputs, i.e.</p> \[S^{t} = \rho^t(F, L^{T_p}, S^{t-1}), \forall T_p \lt t \le T_p + T_c\] <p>where $T_c$ is number of confidence map estimation stages and $\rho^t$ is CNNs inference at stage $t$ which estimates part confidence map. We initialize the pose-extractor with OpenPose pretrained on COCO dataset. Note that we do not need ground-truth pose estimations of SoccerNet-v3 because we only optimize the re-identification loss.</p> <p>We trained the model for 50 epochs, with 32 batch size and 10% percentage (of unique person IDs) of SoccerNet-V3 data. As can be seen in the below table we were able to surpass the OSNET performance and other baselines. Adding Layer-wise similarity (described in later section) and adding channel and/or spatial attention could further increase the performance.</p> <div style="display: flex; justify-content: center;"> <table style="border-collapse: collapse; width: 50%; text-align: center;"> <thead> <tr> <th style="border: 1px solid #ddd; padding: 8px;">Model</th> <th style="border: 1px solid #ddd; padding: 8px;">mAP (%)</th> <th style="border: 1px solid #ddd; padding: 8px;">Rank-1 (%)</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid #ddd; padding: 8px;">OSNET</td> <td style="border: 1px solid #ddd; padding: 8px;">61.6</td> <td style="border: 1px solid #ddd; padding: 8px;">51.2</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">inceptionv4</td> <td style="border: 1px solid #ddd; padding: 8px;">46.7</td> <td style="border: 1px solid #ddd; padding: 8px;">32</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">RESNET50mid</td> <td style="border: 1px solid #ddd; padding: 8px;">46.5</td> <td style="border: 1px solid #ddd; padding: 8px;">31.7</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">RESNET50</td> <td style="border: 1px solid #ddd; padding: 8px;">46.7</td> <td style="border: 1px solid #ddd; padding: 8px;">32.8</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">Ours</td> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">63.7</td> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">52.9</td> </tr> </tbody> </table> </div> <p><em><center>Results (mean average precision and Rank-1 accuracy) on 10% SoccerNet-v3 data with batch size 32.</center></em><br/></p> <h4 id="layer-wise-similarity"><strong>Layer-wise similarity</strong></h4> <p>Before diving into the idea of layer-wise similarity, let me talk little about the metric learning and similarity loss first. Metric learning is a task of machine learning in which the loss to be minimized is a distance between data points. Similarity loss in this context is any type of loss which measures the similarity between images (anchor and any other image). Triplet loss is one of the most frequently used metric learning losses in the re-identification. It is formally defined as,</p> <p> $$ L = max(d(a, p) - d(a, n) + \delta, 0) $$ where $d$ is any distance metric such as euclidean or manhattan distance. We use $L2$ distance. $a$ is an anchor image, $p$ is a positive image, $n$ is a negative image. $\delta$ is a margin. Minimizing the triplet loss in a training has an effect of pushing away negative samples and bringing the positive samples closer simultaneously, as illustrated in the below image. </p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/triplet_loss_analogy.png"/> </p> </html> <p><em><center>Triplet loss in training <a class="citation" href="#facenet">(Schroff et al., 2015)</a>.</center></em><br/> RESNETs have been champions in almost all computer vision tasks from much of their inception. Unsurprisingly, RESNET was also implemented in official <a href="https://github.com/SoccerNet/sn-reid">SoccerNet reidentification developement kit</a>. Although, RESNET’s performance was no way near the state-of-the-art methods their wide use steered us to use it as a backbone model to build upon. For the reasons stated earlier it was desired that we look at what features in the image RESNET was focusing on. Below is activation map of some of the middle layers of the RESNET.</p> <html> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/featuremap.png" width="900"/> </p> </html> <p><em><center>Activation map of RESNET on Soccernet (top left to bottom right : Layer 1 to 6).</center></em></p> <p>In every part out of 6 parts of the activation map image above, there are total of three smaller images. First is a bounding box image which is an input to the respective RESNET layer, second is output (activation map) of the respective RESNET layer and the last is superimposition of activation map on the input to highlight the feature each layer is focusing on. As can be seen, the output/last layer of RESNET was focusing on small spatial features such as shoes in this case, although features such as jersey number were detected in earlier layers. This is one of the drawbacks of CNN based reid models where pooling and strided convolutions reduce the size of output feature maps. Therefore, the idea was to use detected features at every layer in the model to calculate the similarity loss. Doing so would steer model to recognize image as a positive image if not only final feature-maps but also feature-maps at middle layers of the model are largely similar (and vice-versa) to respective feature-maps of the anchor image.</p> <p>Below image illustrates the design of the layer wise similarity in the model,</p> <p><img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/layerwise_similarity.png" alt="Layer wise similarity" title="layer-wise similarity"/> <em><center>Layer-wise similarity</center></em><br/></p> <p>We add FC layers at the end of the RESNET layers to be taken for calculating the layer-wise similarity. We calculate the similarity loss at the output of the FC layers. The total loss is addition of constituent losses at each layer. The number of FC layers and which layers of RESNET to use is chosen based on the validation. Results on the 10% Soccernet data with batch size of 32 with RESNET as backbone show 3.2% improvement in Rank-1 accuracy and 3.7% increase in mAP.</p> <div style="display: flex; justify-content: center;"> <table style="border-collapse: collapse; width: 50%; text-align: center;"> <thead> <tr> <th style="border: 1px solid #ddd; padding: 8px;">Model</th> <th style="border: 1px solid #ddd; padding: 8px;">mAP (%)</th> <th style="border: 1px solid #ddd; padding: 8px;">Rank-1 (%)</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Resnet Baseline</td> <td style="border: 1px solid #ddd; padding: 8px;">46.7</td> <td style="border: 1px solid #ddd; padding: 8px;">32.8</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">Layerwise similarity</td> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">50.4</td> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">36.2</td> </tr> </tbody> </table> </div> <p><em><center>Results (mean average precision and Rank-1 accuracy) of Layer-wise similarity on 10% SoccerNet-v3 data and batch size=32</center></em><br/></p> <p>One of the further areas of exploration is to utilize pose features which are view invariant such as mentioned in View-Invariant Probabilistic Embedding for Human Pose <a class="citation" href="#DBLP:journals/corr/abs-1912-01001">(Sun et al., 2019)</a>. Also, such positional embedding can be effectively utilized with SIE module of TransReid.</p> <p>Ok, that’s it.</p> <p>Finally, I am grateful to the support of <a href="https://cse.buffalo.edu/~doermann/CV.html">Dr. David Doermann</a> - for providing resources required for this project. This project-work was done with equal contributions from <a href="https://www.linkedin.com/in/maheshsbhosale/">Mahesh Bhosale</a> and <a href="https://www.linkedin.com/in/akumar58/">Abhishek Kumar</a>.</p> <p><strong>Resources</strong>:<br/> <a href="https://www.soccer-net.org/">SoccerNet challenge page</a><br/> <a href="https://github.com/SoccerNet/sn-reid">SoccerNet development kit</a><br/> <a href="https://github.com/abhinine4/Soccerplayer_Reidentification">Our Github Repo</a><br/> <a href="https://github.com/abhinine4/Soccerplayer_Reidentification/blob/main/images/soccer_player_reid_report.pdf">Our report</a><br/></p> <p><strong>References</strong><br/></p> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="https://doi.org/10.48550/arxiv.2102.04378" class="col-sm-8"> <div class="title">TransReID: Transformer-based Object Re-Identification</div> <div class="author"> Shuting He,&nbsp;Hao Luo,&nbsp;Pichao Wang, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Fan Wang, Hao Li, Wei Jiang' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="https://doi.org/10.48550/arxiv.2010.11929" class="col-sm-8"> <div class="title">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</div> <div class="author"> Alexey Dosovitskiy,&nbsp;Lucas Beyer,&nbsp;Alexander Kolesnikov, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="zhou2019osnet" class="col-sm-8"> <div class="title">Omni-Scale Feature Learning for Person Re-Identification</div> <div class="author"> Kaiyang Zhou,&nbsp;Yongxin Yang,&nbsp;Andrea Cavallaro, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Tao Xiang' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In ICCV</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="DBLP:journals/corr/abs-1912-01001" class="col-sm-8"> <div class="title">View-Invariant Probabilistic Embedding for Human Pose</div> <div class="author"> Jennifer J. Sun,&nbsp;Jiaping Zhao,&nbsp;Liang-Chieh Chen, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Florian Schroff, Hartwig Adam, Ting Liu' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>CoRR</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Suh_2018_ECCV" class="col-sm-8"> <div class="title">Part-Aligned Bilinear Representations for Person Re-Identification</div> <div class="author"> Yumin Suh,&nbsp;Jingdong Wang,&nbsp;Siyu Tang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Tao Mei, Kyoung Mu Lee' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the European Conference on Computer Vision (ECCV)</em> , Sep 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="DBLP:journals/corr/abs-1812-08008" class="col-sm-8"> <div class="title">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</div> <div class="author"> Zhe Cao,&nbsp;Gines Hidalgo,&nbsp;Tomas Simon, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Shih-En Wei, Yaser Sheikh' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>CoRR</em>, Sep 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="zheng2015scalable" class="col-sm-8"> <div class="title">Scalable Person Re-identification: A Benchmark</div> <div class="author"> Liang Zheng,&nbsp;Liyue Shen,&nbsp;Lu Tian, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Shengjin Wang, Jingdong Wang, Qi Tian' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Computer Vision, IEEE International Conference on</em> , Sep 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="facenet" class="col-sm-8"> <div class="title">FaceNet: A unified embedding for face recognition and clustering</div> <div class="author"> Florian Schroff,&nbsp;Dmitry Kalenichenko,&nbsp;and&nbsp;James Philbin </div> <div class="periodical"> <em>In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> , Sep 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <p><br/></p> <div id="disqus_thread"></div> <script type="text/javascript">var disqus_shortname="bhosalems-github-io";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>]]></content><author><name></name></author><category term="technical"/><category term="video-understanding,"/><category term="reidentification"/><summary type="html"><![CDATA[Soccer Reidentification]]></summary></entry></feed>