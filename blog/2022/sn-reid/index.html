<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SoccerNet Player Re-identification | Mahesh's webpage </title> <meta name="author" content="Mahesh B"> <meta name="description" content="Soccer Reidentification"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" integrity="sha256-VwMV//xgBPDyRFVSOshhRhzJRDyBmIACniLPpeXNUdc=" crossorigin="anonymous"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bhosalems.github.io/blog/2022/sn-reid/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Mahesh's webpage </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">curriculum-vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">SoccerNet Player Re-identification</h1> <p class="post-meta"> Created in August 20, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/video-understanding"> <i class="fa-solid fa-hashtag fa-sm"></i> video-understanding,</a>   <a href="/blog/tag/reidentification"> <i class="fa-solid fa-hashtag fa-sm"></i> reidentification</a>   ·   <a href="/blog/category/technical"> <i class="fa-solid fa-tag fa-sm"></i> technical</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Last semester as a part of CSE 610 Sports Video Analytics class, we worked on the <a href="https://www.soccer-net.org/tasks/re-identification" rel="external nofollow noopener" target="_blank">Soccernet-Player Re-identification</a> challenge. Below are the notes from the work done in this project.</p> <h4 id="whats-the-task-of-person-re-identification"><strong>What’s the task of Person Re-identification?</strong></h4> <p>The task of Person Re-identification can be formulated differently leading to multiple definitions. I will start with one which is quite straightforward, and introduce others later. As name suggests it is all about “Re” identifying the person. More precisely, person Re-identification is a task of identifying the same person in two time and/or view disjoint frames taken from multiple cameras. <br></p> <p>Below is an image from person re-identification dataset called Market-1501<a class="citation" href="#zheng2015scalable">(Zheng et al., 2015)</a>. It contains in total 8 sequence of images(3 sequences in first two rows and two sequences in the last row. Note, here sequence doesn’t necessarily mean any order between images, it just is a collection), each image in a sequence is of the same person/identity taken from multiple views captured by different cameras in a market. The task of person re-identification is to build the correspondence between images in the same sequence. Also, as can be seen in the laarketst row, there can also be negative examples i.e. there is either no sufficient information to identify the person or there’s no more images to retrieve (only single reference image). Based on requirements of a task at hand, output in such cases could be different e.g. “none” if similarity score/ other metric is below some reasonable threshold value.<br></p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/Market-1501.png" width="900"> </p> <p><em><center>Market 1501 Dataset <a class="citation" href="#zheng2015scalable">(Zheng et al., 2015)</a>.</center></em> <br> Having gone over what person re-identification is, player re-identification is self-explanatory : person re-identification done for players in particular sports.</p> <p>Mostly all publicly available person/player re-identification datasets have cropped images of players from image frames in the video. These videos can be from different geographic locations and timestamps, captured from multiple cameras with dissimilar views/orientations. Although timestamps are different, difference is smaller (in minutes or less). Multiple camera views pose a challenge because the views are disjoint, temporal distance between images is not constant, lighting conditions and backgrounds are different.</p> <p>Given such dataset, you will find some standard definitions in literature which I will introduce here. <br></p> <ul> <li>Anchor/Query Image : An image of the target player to be re-identified.<br> </li> <li>Action frame : A frame from which an anchor image is captured. In case of SoccerNet dataset, action signifies some interesting event in the soccer e.g. a goal. All the frames in the video for that action are grouped together and used for evaluation as these frames are temporally closer.<br> </li> <li>Reference frame: All other frames (can be from same or different action) in the dataset except the action frame.<br> </li> <li>Gallery set: Nothing fancy, it simply is a set of all reference frames / bounding boxes(likely have different views/timestamps).<br> </li> <li>Positive Image: An image of the same identity as that in the anchor image.<br> </li> <li>Negative Image: An image which doesn’t have the same identity as that in the anchor image.<br> </li> </ul> <p>Below is the illustration of Soccernet-v3 dataset -<br></p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/soccernet-v3-reid-illustration.png" width="950" height="600"> </p> <p><em><center>Soccernet Re-identification Dataset Illustration</center></em> <br> Let’s say above image is divided into four parts by two dotted lines. Top left corner is an action frame, most likely a goal. After that, in top right you have reference frames named as replay frames; notice the small temporal distance between action and replay frames. In the lower-left part of the image, 18 bounding boxes are captured, each will be used as (possible) query image, and all the 37 bounding boxes captured from the reference images will form a gallery, shown in the lower right part. In some cases, such as the bottom-most image of a referee in the queries doesn’t have any matching image in the gallery set, then it will be moved to gallery set to create a distraction. Having taken a look at some images in the dataset, take a minute to think about the challenges I mentioned in the dataset such as difference in background, resolution and size.</p> <p>So in summary, an anchor image is taken from action frames; positive images and negative images are from gallery set. You are given an anchor image, your model needs to find the positive image from the gallery set. With above description you could also formulate the problem of re-identification in terms of image retrieval based on metric learning. I will talk more about metric learning in later section.</p> <h4 id="state-of-the-art-methods-of-re-identification"><strong>State-of-the-Art methods of Re-identification</strong></h4> <p>Current methods of player re-identification mainly focus on two ways - one is to get high quality discriminative features and other is to define the distance metric which can be used as a loss for learning task effectively. I will talk briefly about the prior as it aligned with the requirements of the class. While learning the features from the images, it is important to work on relatively similar scale of the images. In more open-world settings, distance of objects from the camera is different, and so ae there sizes in the image; although to some extent it is taken care by cropping the image and resizing the images to same size, it is important to construct features from different scales in an image. One of such methods exploring the ideas of multiple scales is from the authors of OSNET<a class="citation" href="#zhou2019osnet">(Zhou et al., 2019)</a>. OsNET was also SOTA method in <a href="https://github.com/SoccerNet/sn-reid/tree/main/torchreid/models" rel="external nofollow noopener" target="_blank">SoccerNet baselines</a> which gave the best results. One of the other methods that we reviewed was recent addition to SOTAs, a transformer based re-identification model - TransReid. Let me describe both OSNET and TransReid in following sections.</p> <p><strong>1. Omni-Scale Feature Learning for Person Re-Identification (OSNET)</strong> <a class="citation" href="#zhou2019osnet">(Zhou et al., 2019)</a><br> Authors of this paper argue that to match the people, small local features (e.g. shoes, purse etc.) and relatively larger global features (e.g. whole body appearance) are equally important. Therefore, such discriminative features should be <em>omniscale</em>, defined as the combination of variable homogeneous scales and heterogeneous scales, each of which is composed of a mixture of multiple scales.</p> <p>Authors propose a novel CNN architecture OSNET. The main idea is to have multiple CNN streams with different receptive fields so that the multiscale features can be learnt. At last, resulting multiscale feature maps from each stream are fused by weighted aggregation gate (AG). The AG is a mini-network sharing parameters across all the CNN streams. With the trainable AG, the generated channel-wise weights become input-dependent, hence the dynamic scale fusion. There are some more ideas adapted in this paper such as <a href="http://stanford.edu/class/ee367/Winter2019/bergman_report.pdf" rel="external nofollow noopener" target="_blank">depth-wise convolutions</a> to make the module light-weight. For more detailed understanding reader is advised to review OSNET paper <a class="citation" href="#zhou2019osnet">(Zhou et al., 2019)</a>.</p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/osnet-arch.png"> </p> <p><em><center>OSNET bottleneck block</center></em> <br></p> <p><strong>2. TransReid: Transformer-based Object Re-Identification</strong> <a class="citation" href="#https://doi.org/10.48550/arxiv.2102.04378">(He et al., 2021)</a><br> One of the other methods which although, doesn’t directly encode multiscale features it does address main problems with CNN based Re-ID methods. I have seen TransReid perform quite well in the task of player re-identification, which should come with no surprise as Transformer based models are performing better and better stacking up hundreds of submissions in the top conferences these days.</p> <p>There are two main problems with the traditional CNN based approaches of re-identification - 1. CNN based methods focus on small discriminative features due to a Gaussian distribution of effective receptive fields. 2. Down-sampling operators of CNN reduce the spatial dimension of the feature-map (as you would also see that this was one of the motivations for us to use Layer-wise similarity discussed in the later section).</p> <p>Authors of TransReid propose to address these issues -<br> Use of attention captures long range dependencies as complete global information is available at each layer despite its depth. Without down-sampling operators, transformers can keep more detailed information. To further add robust features authors introduce two modules -</p> <ol> <li>Jigsaw patches module : As with vision transformer, <a class="citation" href="#https://doi.org/10.48550/arxiv.2010.11929">(Dosovitskiy et al., 2020)</a> the image is split into fixed sized patches and attention based mechanism is used to learn the features. This module attempts to rearrange the patch embeddings via shift and shuffle operations and regroup them for further feature learning. This enables robustness in the learned features and also expands on long-range dependencies.</li> <li>Side information embedding : In many of the re-identification datasets we have non-visual information which can not be processed by purely CNN based model. Therefore, there is no way of addressing data bias brought by cameras or viewpoints. This module, similar to position encoding in vision transformer, uses learnable 1D embeddings to encode side information suh as camera and view metadata.</li> </ol> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/trans-reid-arch.png" width="900"> </p> <p><em><center>TransReid architecture</center></em> <br></p> <p>For more detailed treatment of TransReid reader is advised to review TransReid paper <a class="citation" href="#https://doi.org/10.48550/arxiv.2102.04378">(He et al., 2021)</a></p> <h4 id="appearance-and-pose-as-discriminative-features"><strong>Appearance and Pose as discriminative features</strong></h4> <p><strong>Motivation:</strong><br> As compared to the task of person re-identification, the task of player re-identification is significantly challenging. Many methods base their model on appearance as discriminative feature to learn the metrics, but in case of players, appearance of almost all is similar - for example in a game of football the general physique of all the players would be on average similar. Almost all players from same team will wear similar jersey, exception being goal-keepers, but there is only one goalkeeper in a team on the field. Now, you might be able to identify player based on their jersey numbers but remember we have to re-identify players from different camera views, and it is more likely than not that the jersey numbers are either not visible in the given view or too obscure to even be detected let alone be identified as can be seen from below pictures.</p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/obscure_imgs.png" width="100" height="200"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/obscure_imgs1.png" width="100" height="200"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/obscure_imgs2.png" width="100" height="200"> </p> <p><em><center>Soccernet-v3 images with obscure Jersey numbers </center></em> <br> Therefore, appearance features alone are not sufficient. It is also evident from the difference in the performance of the SOTA methods such as OSNET on person re-identification vs player re-identification.<br><br></p> <div style="display: flex; justify-content: center;"> <table style="border-collapse: collapse; width: 50%; text-align: center;"> <thead> <tr> <th style="border: 1px solid #ddd; padding: 8px;">Dataset</th> <th style="border: 1px solid #ddd; padding: 8px;">mAP (%)</th> <th style="border: 1px solid #ddd; padding: 8px;">Rank-1 (%)</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Person re-identification (Market1501)</td> <td style="border: 1px solid #ddd; padding: 8px;">81</td> <td style="border: 1px solid #ddd; padding: 8px;">93.6</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Player re-identification (SoccerNet-v3)</td> <td style="border: 1px solid #ddd; padding: 8px;">61.6</td> <td style="border: 1px solid #ddd; padding: 8px;">51.2</td> </tr> </tbody> </table> </div> <p><em><center>Performance of SOTA OSNET on person vs player re-identification (mean average precision and Rank-1 accuracy)<a class="citation" href="#zhou2019osnet">(Zhou et al., 2019)</a>. </center></em></p> <p>One of the main difference we identified in traditional person re-identification datasets and SoccerNet dataset is that temporal distance between anchor image and reference images in case of SoccerNet dataset is much smaller than that of person re-identification datasets. Which means a player in anchor image and the same player in (positive) reference images is likely to have similar body posture. Also, in almost every team-sport, based on the role of a player in overall game, there are distinct moves that they do at a given time. Posture of players therefore, could be used as additional discriminative feature to guide the task of metric learning. This was one of the main ideas that we implemented in the project.</p> <p><strong>Methodology:</strong><br> We need to extract both posture features and appearance features from the input image; We use two-stream model where one stream is called as appearance extractor which works on extracting the appearance features from the images, and second stream called as part/pose extractor works on extracting the pose related features from the images. We use RESNET-50 as appearance extractor and sub-model of openpose as pose-extractor. At the end we need to combine both the appearance and pose features to calculate the final loss. We use (compact) bi-linear pooling to pool the features from both the streams. Our choice of pose extractor and pooling has been adapted from Part-aligned bilinear pooling for re-identification paper<a class="citation" href="#Suh_2018_ECCV">(Suh et al., 2018)</a>.</p> <p>Below image shows the two-stream extractor -<br></p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/model_arch.png" width="900"> </p> <p><em><center>Two stream (appearance and pose) extractor model architecture. </center></em></p> <p>Note that we use two losses, first is Triplet loss as a similarity loss which will be explained in detail in a later section. Another is an identity loss, this is nothing but a traditional cross-entropy loss used in classification tasks. It is formally given as,</p> \[L = \frac{1}{m} \sum_{i=1}^m y_i \dot{} \log{\hat {y}_i}\] <p><strong>OpenPose:</strong><br> Let me briefly describe main concepts in OpenPose and the sub-model that we use in our work. OpenPose<a class="citation" href="#DBLP:journals/corr/abs-1812-08008">(Cao et al., 2018)</a> is the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints (total 135 keypoints).</p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/pose_face_hands.gif"> </p> <p style="text-align:center;"><em>Authors of OpenPose: Ginés Hidalgo (left) and Hanbyul Joo (right) in front of the CMU Panoptic Studio.<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="external nofollow noopener" target="_blank"> image source</a></em></p> <p>There are mainly two approaches in multi-person 2D human pose detection - <br></p> <ol> <li>Top-down approach - In top-down approach, a single person is detected first, and then the pose is estimated for every such detection.</li> <li>Bottom-up approach - On the contrary, in bottom-up approach, local features (such as body parts) are detected and associated with each other to get the global context/information about complete pose.</li> </ol> <p>OpenPose is based off Bottom-up approach. Figure below visualizes the complete pipeline of the OpenPose.</p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/Openpose_pipeline.png" width="900"> </p> <p><em><center>OpenPose pipeline <a class="citation" href="#DBLP:journals/corr/abs-1812-08008">(Cao et al., 2018)</a> </center></em></p> <p>OpenPose takes in 2D color image of size $H \times W$ as input (Fig. a) and produces anatomical key points on each person in the image as output (Fig. e). First, feed-forward network predicts set of confidence maps $S$ of body parts (Fig. b) and set of 2D vector fields $L$ called as part affinity fields (PAF), which encode degree of association between body parts (Fig. c). The set $S=(S_1, S_2, …, S_J)$ has $J$ confidence maps, one per part, where $S_j \in \mathbb{R}^{w \times h}$, $j \in {1 . . . J}$. The set $L=(L_1,L_2, …,L_C )$ has $C$ vector fields one per limb (including face, although technically it’s not a limb) where $L_c \in \mathbb{R}^{w \times h \times 2}$, $c \in {1, ..C}$. Once all the PAFs and confidence maps are identified, bipartite matching does the association and the result is 2D key-points for all people in the image (Fig. d). Note that each image location in L encodes a 2D vector as shown in the below figure.</p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/PAFs.png"> </p> <p><em><center> (left) Part Affinity Fields (PAFs) corresponding to the limb connecting right elbow and wrist. The color encodes orientation. (right) A 2D vector in each pixel of every PAF encodes the position and orientation of the limbs. <a class="citation" href="#DBLP:journals/corr/abs-1812-08008">(Cao et al., 2018)</a> </center></em></p> <p>As stated earlier, we only need a sub-model of the OpenPose, particularly, the part until it calculates the final part confidence features which we use for bi-linear pooling with appearance features. Multi-stage architecture of OpenPose is given in the below figure, where first stages predict PAFs and later stages predict the part confidence maps.</p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/OpenPose_arch.png"> </p> <p><em><center> Multistage OpenPose Architecture</center></em></p> <p>First, image is fed into pretrained VGG-19, which gives the feature maps $F$ that is input to the first stage and outputs the first PAF. In each subsequent stage, the predicted PAF from the previous stage and the original image feature map $F$ are concatenated and used to produce the refined predictions. Formally first PAF $L^1$ is calculated as,</p> \[L^1 = \phi^1(F)\] <p>Subsequent PAFs are calculated as,</p> \[L^t = \phi^t(F, L^{t-1}), \forall 2\le t \le T_p\] <p>where, $\phi^t$ refers to CNNs inference at stage $t$. After total of $T_p$ PAF stages, last PAF is given as input to next stage for estimating part confidence map. First stage only takes $L^{T_p}$ and $F$ as inputs, i.e.</p> \[S^{T_p} = \rho^t(F, L^{T_p}), \forall t=T_p\] <p>whereas subsequent stages take $L^{T_p}$, $F$ and $S^{T-1}$ as inputs, i.e.</p> \[S^{t} = \rho^t(F, L^{T_p}, S^{t-1}), \forall T_p \lt t \le T_p + T_c\] <p>where $T_c$ is number of confidence map estimation stages and $\rho^t$ is CNNs inference at stage $t$ which estimates part confidence map. We initialize the pose-extractor with OpenPose pretrained on COCO dataset. Note that we do not need ground-truth pose estimations of SoccerNet-v3 because we only optimize the re-identification loss.</p> <p>We trained the model for 50 epochs, with 32 batch size and 10% percentage (of unique person IDs) of SoccerNet-V3 data. As can be seen in the below table we were able to surpass the OSNET performance and other baselines. Adding Layer-wise similarity (described in later section) and adding channel and/or spatial attention could further increase the performance.</p> <div style="display: flex; justify-content: center;"> <table style="border-collapse: collapse; width: 50%; text-align: center;"> <thead> <tr> <th style="border: 1px solid #ddd; padding: 8px;">Model</th> <th style="border: 1px solid #ddd; padding: 8px;">mAP (%)</th> <th style="border: 1px solid #ddd; padding: 8px;">Rank-1 (%)</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid #ddd; padding: 8px;">OSNET</td> <td style="border: 1px solid #ddd; padding: 8px;">61.6</td> <td style="border: 1px solid #ddd; padding: 8px;">51.2</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">inceptionv4</td> <td style="border: 1px solid #ddd; padding: 8px;">46.7</td> <td style="border: 1px solid #ddd; padding: 8px;">32</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">RESNET50mid</td> <td style="border: 1px solid #ddd; padding: 8px;">46.5</td> <td style="border: 1px solid #ddd; padding: 8px;">31.7</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px;">RESNET50</td> <td style="border: 1px solid #ddd; padding: 8px;">46.7</td> <td style="border: 1px solid #ddd; padding: 8px;">32.8</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">Ours</td> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">63.7</td> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">52.9</td> </tr> </tbody> </table> </div> <p><em><center>Results (mean average precision and Rank-1 accuracy) on 10% SoccerNet-v3 data with batch size 32.</center></em><br></p> <h4 id="layer-wise-similarity"><strong>Layer-wise similarity</strong></h4> <p>Before diving into the idea of layer-wise similarity, let me talk little about the metric learning and similarity loss first. Metric learning is a task of machine learning in which the loss to be minimized is a distance between data points. Similarity loss in this context is any type of loss which measures the similarity between images (anchor and any other image). Triplet loss is one of the most frequently used metric learning losses in the re-identification. It is formally defined as,</p> <p> $$ L = max(d(a, p) - d(a, n) + \delta, 0) $$ where $d$ is any distance metric such as euclidean or manhattan distance. We use $L2$ distance. $a$ is an anchor image, $p$ is a positive image, $n$ is a negative image. $\delta$ is a margin. Minimizing the triplet loss in a training has an effect of pushing away negative samples and bringing the positive samples closer simultaneously, as illustrated in the below image. </p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/triplet_loss_analogy.png"> </p> <p><em><center>Triplet loss in training <a class="citation" href="#facenet">(Schroff et al., 2015)</a>.</center></em><br> RESNETs have been champions in almost all computer vision tasks from much of their inception. Unsurprisingly, RESNET was also implemented in official <a href="https://github.com/SoccerNet/sn-reid" rel="external nofollow noopener" target="_blank">SoccerNet reidentification developement kit</a>. Although, RESNET’s performance was no way near the state-of-the-art methods their wide use steered us to use it as a backbone model to build upon. For the reasons stated earlier it was desired that we look at what features in the image RESNET was focusing on. Below is activation map of some of the middle layers of the RESNET.</p> <p style="text-align:center;"> <img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/featuremap.png" width="900"> </p> <p><em><center>Activation map of RESNET on Soccernet (top left to bottom right : Layer 1 to 6).</center></em></p> <p>In every part out of 6 parts of the activation map image above, there are total of three smaller images. First is a bounding box image which is an input to the respective RESNET layer, second is output (activation map) of the respective RESNET layer and the last is superimposition of activation map on the input to highlight the feature each layer is focusing on. As can be seen, the output/last layer of RESNET was focusing on small spatial features such as shoes in this case, although features such as jersey number were detected in earlier layers. This is one of the drawbacks of CNN based reid models where pooling and strided convolutions reduce the size of output feature maps. Therefore, the idea was to use detected features at every layer in the model to calculate the similarity loss. Doing so would steer model to recognize image as a positive image if not only final feature-maps but also feature-maps at middle layers of the model are largely similar (and vice-versa) to respective feature-maps of the anchor image.</p> <p>Below image illustrates the design of the layer wise similarity in the model,</p> <p><img src="/assets/img/blogs/2022-07-09-Excerpt-on-Reidentification/layerwise_similarity.png" alt="Layer wise similarity" title="layer-wise similarity"> <em><center>Layer-wise similarity</center></em><br></p> <p>We add FC layers at the end of the RESNET layers to be taken for calculating the layer-wise similarity. We calculate the similarity loss at the output of the FC layers. The total loss is addition of constituent losses at each layer. The number of FC layers and which layers of RESNET to use is chosen based on the validation. Results on the 10% Soccernet data with batch size of 32 with RESNET as backbone show 3.2% improvement in Rank-1 accuracy and 3.7% increase in mAP.</p> <div style="display: flex; justify-content: center;"> <table style="border-collapse: collapse; width: 50%; text-align: center;"> <thead> <tr> <th style="border: 1px solid #ddd; padding: 8px;">Model</th> <th style="border: 1px solid #ddd; padding: 8px;">mAP (%)</th> <th style="border: 1px solid #ddd; padding: 8px;">Rank-1 (%)</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid #ddd; padding: 8px;">Resnet Baseline</td> <td style="border: 1px solid #ddd; padding: 8px;">46.7</td> <td style="border: 1px solid #ddd; padding: 8px;">32.8</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">Layerwise similarity</td> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">50.4</td> <td style="border: 1px solid #ddd; padding: 8px; font-weight: bold;">36.2</td> </tr> </tbody> </table> </div> <p><em><center>Results (mean average precision and Rank-1 accuracy) of Layer-wise similarity on 10% SoccerNet-v3 data and batch size=32</center></em><br></p> <p>One of the further areas of exploration is to utilize pose features which are view invariant such as mentioned in View-Invariant Probabilistic Embedding for Human Pose <a class="citation" href="#DBLP:journals/corr/abs-1912-01001">(Sun et al., 2019)</a>. Also, such positional embedding can be effectively utilized with SIE module of TransReid.</p> <p>Ok, that’s it.</p> <p>Finally, I am grateful to the support of <a href="https://cse.buffalo.edu/~doermann/CV.html" rel="external nofollow noopener" target="_blank">Dr. David Doermann</a> - for providing resources required for this project. This project-work was done with equal contributions from <a href="https://www.linkedin.com/in/maheshsbhosale/" rel="external nofollow noopener" target="_blank">Mahesh Bhosale</a> and <a href="https://www.linkedin.com/in/akumar58/" rel="external nofollow noopener" target="_blank">Abhishek Kumar</a>.</p> <p><strong>Resources</strong>:<br> <a href="https://www.soccer-net.org/" rel="external nofollow noopener" target="_blank">SoccerNet challenge page</a><br> <a href="https://github.com/SoccerNet/sn-reid" rel="external nofollow noopener" target="_blank">SoccerNet development kit</a><br> <a href="https://github.com/abhinine4/Soccerplayer_Reidentification" rel="external nofollow noopener" target="_blank">Our Github Repo</a><br> <a href="https://github.com/abhinine4/Soccerplayer_Reidentification/blob/main/images/soccer_player_reid_report.pdf" rel="external nofollow noopener" target="_blank">Our report</a><br></p> <p><strong>References</strong><br></p> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="https://doi.org/10.48550/arxiv.2102.04378" class="col-sm-8"> <div class="title">TransReID: Transformer-based Object Re-Identification</div> <div class="author"> Shuting He, Hao Luo, Pichao Wang, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Fan Wang, Hao Li, Wei Jiang' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="https://doi.org/10.48550/arxiv.2010.11929" class="col-sm-8"> <div class="title">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</div> <div class="author"> Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="zhou2019osnet" class="col-sm-8"> <div class="title">Omni-Scale Feature Learning for Person Re-Identification</div> <div class="author"> Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Tao Xiang' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In ICCV</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="DBLP:journals/corr/abs-1912-01001" class="col-sm-8"> <div class="title">View-Invariant Probabilistic Embedding for Human Pose</div> <div class="author"> Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Florian Schroff, Hartwig Adam, Ting Liu' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>CoRR</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Suh_2018_ECCV" class="col-sm-8"> <div class="title">Part-Aligned Bilinear Representations for Person Re-Identification</div> <div class="author"> Yumin Suh, Jingdong Wang, Siyu Tang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Tao Mei, Kyoung Mu Lee' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the European Conference on Computer Vision (ECCV)</em> , Sep 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="DBLP:journals/corr/abs-1812-08008" class="col-sm-8"> <div class="title">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</div> <div class="author"> Zhe Cao, Gines Hidalgo, Tomas Simon, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Shih-En Wei, Yaser Sheikh' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>CoRR</em>, Sep 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="zheng2015scalable" class="col-sm-8"> <div class="title">Scalable Person Re-identification: A Benchmark</div> <div class="author"> Liang Zheng, Liyue Shen, Lu Tian, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Shengjin Wang, Jingdong Wang, Qi Tian' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Computer Vision, IEEE International Conference on</em> , Sep 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="facenet" class="col-sm-8"> <div class="title">FaceNet: A unified embedding for face recognition and clustering</div> <div class="author"> Florian Schroff, Dmitry Kalenichenko, and James Philbin </div> <div class="periodical"> <em>In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> , Sep 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <p><br></p> <div id="disqus_thread"></div> <script type="text/javascript">var disqus_shortname="bhosalems-github-io";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/kl_distance/">Distance metric for fairness in vision language models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/frequent_commands/">Quotodian commands</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/diff/">Diffusion Models Background and Code</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mahesh B. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://www.linkedin.com/in/maheshsbhosale/" target="_blank" rel="external nofollow noopener">Mahesh</a>. Last updated: January 21, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0}};</script> <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-YyRhy+F3bG+zvvwsW1E/uy5s5pKXwmG/bOPlV+1pt6o=" crossorigin="anonymous"></script> <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.js" integrity="sha256-aVkDxqyzrB+ExUsOY9PdyelkDhn/DfrjWu08aVpqNlo=" crossorigin="anonymous"></script> <script>document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-pseudocode").forEach(e=>{const t=e.textContent,d=e.parentElement.parentElement;let n=document.createElement("pre");n.classList.add("pseudocode");const o=document.createTextNode(t);n.appendChild(o),d.appendChild(n),d.removeChild(e.parentElement),pseudocode.renderElement(n)})});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-curriculum-vitae",title:"curriculum-vitae",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-teaching",title:"teaching",description:"Several CSE courses I have been a teaching assistant for",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-distance-metric-for-fairness-in-vision-language-models",title:"Distance metric for fairness in vision language models",description:"Distance metric for fairness",section:"Posts",handler:()=>{window.location.href="/blog/2024/kl_distance/"}},{id:"post-quotodian-commands",title:"Quotodian commands",description:"Trace of all commands that use daily.",section:"Posts",handler:()=>{window.location.href="/blog/2024/frequent_commands/"}},{id:"post-diffusion-models-background-and-code",title:"Diffusion Models Background and Code",description:"Diffusion models and its code in Gaussian diffusion codebase",section:"Posts",handler:()=>{window.location.href="/blog/2024/diff/"}},{id:"post-soccernet-player-re-identification",title:"SoccerNet Player Re-identification",description:"Soccer Reidentification",section:"Posts",handler:()=>{window.location.href="/blog/2022/sn-reid/"}},{id:"news-two-papers-submitted-to-cvpr-2025-including-one-first-author-paper-on-histopathology-image-synthesis",title:"Two papers submitted to CVPR 2025, including one first author paper on histopathology image synthesis.",description:"",section:"News"},{id:"news-one-paper-accepted-to-icdar-2024",title:"One paper accepted to ICDAR 2024.",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0003-7157-2129","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=HcQtx-4AAAAJ&hl","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/bhosalems","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/maheshsbhosale","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/ms_bhosale","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>